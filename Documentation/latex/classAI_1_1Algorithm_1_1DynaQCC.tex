\hypertarget{classAI_1_1Algorithm_1_1DynaQCC}{\section{A\-I\-:\-:Algorithm\-:\-:Dyna\-Q\-C\-C$<$ S, A $>$ Class Template Reference}
\label{classAI_1_1Algorithm_1_1DynaQCC}\index{A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C$<$ S, A $>$@{A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C$<$ S, A $>$}}
}


\hyperlink{classAI_1_1Algorithm_1_1DynaQ}{Dyna\-Q} that performs simulation concurrently.  




{\ttfamily \#include $<$Dyna\-Q\-C\-C.\-h$>$}

Inheritance diagram for A\-I\-:\-:Algorithm\-:\-:Dyna\-Q\-C\-C$<$ S, A $>$\-:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=4.000000cm]{classAI_1_1Algorithm_1_1DynaQCC}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classAI_1_1Algorithm_1_1DynaQCC_aab347f88243e3690cbc856347ed37378}{Dyna\-Q\-C\-C} (\hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{A\-I\-::\-F\-L\-O\-A\-T} step\-Size, \hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{A\-I\-::\-F\-L\-O\-A\-T} discount\-Rate, \hyperlink{classAI_1_1Algorithm_1_1Policy_1_1Policy}{Policy\-::\-Policy}$<$ S, A $>$ \&policy, \hyperlink{namespaceAI_ab6e14dc1e659854858a87e511f1439ec}{A\-I\-::\-U\-I\-N\-T} simulation\-Iteration\-Count, \hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{A\-I\-::\-F\-L\-O\-A\-T} state\-Transition\-Greediness, \hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{A\-I\-::\-F\-L\-O\-A\-T} state\-Transition\-Step\-Size)
\item 
virtual void \hyperlink{classAI_1_1Algorithm_1_1DynaQCC_ae23b8f0afbb9fc5024aef9ce720c9b84}{update} (const \hyperlink{classAI_1_1StateAction}{State\-Action}$<$ S, A $>$ \&current\-State\-Action, const S \&next\-State, const \hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{F\-L\-O\-A\-T} reward, const set$<$ A $>$ \&action\-Set)
\end{DoxyCompactItemize}
\subsection*{Additional Inherited Members}


\subsection{Detailed Description}
\subsubsection*{template$<$class S, class A$>$class A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C$<$ S, A $>$}

\hyperlink{classAI_1_1Algorithm_1_1DynaQ}{Dyna\-Q} that performs simulation concurrently. 


\begin{DoxyTemplParams}{Template Parameters}
{\em S} & State data type. \\
\hline
{\em A} & Action data type.\\
\hline
\end{DoxyTemplParams}
\hyperlink{classAI_1_1Algorithm_1_1DynaQCC}{Dyna\-Q\-C\-C} must only be used when state space is large to take advantage of multi-\/threading. If state space is in order of thousands or more, \hyperlink{classAI_1_1Algorithm_1_1ReinforcementLearningGD}{Reinforcement\-Learning\-G\-D} might should be used. 

\subsection{Constructor \& Destructor Documentation}
\hypertarget{classAI_1_1Algorithm_1_1DynaQCC_aab347f88243e3690cbc856347ed37378}{\index{A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C@{A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C}!Dyna\-Q\-C\-C@{Dyna\-Q\-C\-C}}
\index{Dyna\-Q\-C\-C@{Dyna\-Q\-C\-C}!AI::Algorithm::DynaQCC@{A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C}}
\subsubsection[{Dyna\-Q\-C\-C}]{\setlength{\rightskip}{0pt plus 5cm}template$<$class S , class A $>$ {\bf A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C}$<$ S, A $>$\-::{\bf Dyna\-Q\-C\-C} (
\begin{DoxyParamCaption}
\item[{{\bf A\-I\-::\-F\-L\-O\-A\-T}}]{step\-Size, }
\item[{{\bf A\-I\-::\-F\-L\-O\-A\-T}}]{discount\-Rate, }
\item[{{\bf Policy\-::\-Policy}$<$ S, A $>$ \&}]{policy, }
\item[{{\bf A\-I\-::\-U\-I\-N\-T}}]{simulation\-Iteration\-Count, }
\item[{{\bf A\-I\-::\-F\-L\-O\-A\-T}}]{state\-Transition\-Greediness, }
\item[{{\bf A\-I\-::\-F\-L\-O\-A\-T}}]{state\-Transition\-Step\-Size}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}}\label{classAI_1_1Algorithm_1_1DynaQCC_aab347f88243e3690cbc856347ed37378}

\begin{DoxyParams}{Parameters}
{\em step\-Size} & range $[0.0, 1.0]$j. High step size means faster learning, but less precise convergence. \\
\hline
{\em discount\-Rate} & range $[0.0, 1.0]$. High discount rate means more consideration of future events. \\
\hline
{\em policy} & online policy, that is policy used for action selection. \\
\hline
{\em simulation\-Iteration\-Count} & number of simulations per update/backup. \\
\hline
{\em state\-Transition\-Greediness} & greediness in selecting highest value model. \\
\hline
{\em state\-Transition\-Step\-Size} & how fast does a model update a value of a state-\/action pair. \\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\hypertarget{classAI_1_1Algorithm_1_1DynaQCC_ae23b8f0afbb9fc5024aef9ce720c9b84}{\index{A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C@{A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C}!update@{update}}
\index{update@{update}!AI::Algorithm::DynaQCC@{A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C}}
\subsubsection[{update}]{\setlength{\rightskip}{0pt plus 5cm}template$<$class S , class A $>$ void {\bf A\-I\-::\-Algorithm\-::\-Dyna\-Q\-C\-C}$<$ S, A $>$\-::update (
\begin{DoxyParamCaption}
\item[{const {\bf State\-Action}$<$ S, A $>$ \&}]{current\-State\-Action, }
\item[{const S \&}]{next\-State, }
\item[{const {\bf F\-L\-O\-A\-T}}]{reward, }
\item[{const set$<$ A $>$ \&}]{action\-Set}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}}\label{classAI_1_1Algorithm_1_1DynaQCC_ae23b8f0afbb9fc5024aef9ce720c9b84}
Update the state\-Action map. 
\begin{DoxyParams}{Parameters}
{\em current\-State} & current\-State agent is in. \\
\hline
{\em current\-Action} & action taken by agent by being in current\-State. \\
\hline
{\em next\-State} & next\-State by taking current\-Action in current\-State. \\
\hline
{\em reward} & reward of current\-State\-Action. \\
\hline
{\em action\-Set} & A set of all actions. \\
\hline
\end{DoxyParams}


Reimplemented from \hyperlink{classAI_1_1Algorithm_1_1ReinforcementLearning_a25d7fa245a79e61061436dc0f1db90cb}{A\-I\-::\-Algorithm\-::\-Reinforcement\-Learning$<$ S, A $>$}.



The documentation for this class was generated from the following file\-:\begin{DoxyCompactItemize}
\item 
Algorithms/\-Reinforcement\-Learning/Dyna\-Q\-C\-C.\-h\end{DoxyCompactItemize}
