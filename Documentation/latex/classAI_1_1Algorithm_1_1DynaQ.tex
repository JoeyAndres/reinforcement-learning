\hypertarget{classAI_1_1Algorithm_1_1DynaQ}{\section{A\+I\+:\+:Algorithm\+:\+:Dyna\+Q$<$ S, A $>$ Class Template Reference}
\label{classAI_1_1Algorithm_1_1DynaQ}\index{A\+I\+::\+Algorithm\+::\+Dyna\+Q$<$ S, A $>$@{A\+I\+::\+Algorithm\+::\+Dyna\+Q$<$ S, A $>$}}
}


\hyperlink{classAI_1_1Algorithm_1_1DynaQ}{Dyna\+Q} algorithm implementation.  




{\ttfamily \#include $<$Dyna\+Q.\+h$>$}

Inheritance diagram for A\+I\+:\+:Algorithm\+:\+:Dyna\+Q$<$ S, A $>$\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=4.794520cm]{classAI_1_1Algorithm_1_1DynaQ}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classAI_1_1Algorithm_1_1DynaQ_aa543816270f72b2c4ac1f77fb818f792}{Dyna\+Q} (\hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{A\+I\+::\+F\+L\+O\+A\+T} step\+Size, \hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{A\+I\+::\+F\+L\+O\+A\+T} discount\+Rate, \hyperlink{classAI_1_1Algorithm_1_1Policy_1_1Policy}{Policy\+::\+Policy}$<$ S, A $>$ \&policy, \hyperlink{namespaceAI_ab6e14dc1e659854858a87e511f1439ec}{A\+I\+::\+U\+I\+N\+T} simulation\+Iteration\+Count, \hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{A\+I\+::\+F\+L\+O\+A\+T} state\+Transition\+Greediness, \hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{A\+I\+::\+F\+L\+O\+A\+T} state\+Transition\+Step\+Size)
\item 
virtual void \hyperlink{classAI_1_1Algorithm_1_1DynaQ_a4542226b17db4ed8a2c5ec17d37dc42f}{update} (const \hyperlink{classAI_1_1StateAction}{State\+Action}$<$ S, A $>$ \&current\+State\+Action, const S \&next\+State, const \hyperlink{namespaceAI_a41b74884a20833db653dded3918e05c3}{F\+L\+O\+A\+T} reward, const set$<$ A $>$ \&action\+Set)
\end{DoxyCompactItemize}
\subsection*{Additional Inherited Members}


\subsection{Detailed Description}
\subsubsection*{template$<$class S, class A$>$class A\+I\+::\+Algorithm\+::\+Dyna\+Q$<$ S, A $>$}

\hyperlink{classAI_1_1Algorithm_1_1DynaQ}{Dyna\+Q} algorithm implementation. 


\begin{DoxyTemplParams}{Template Parameters}
{\em S} & State data type. \\
\hline
{\em A} & Action data type.\\
\hline
\end{DoxyTemplParams}
\hyperlink{classAI_1_1Algorithm_1_1DynaQ}{Dyna\+Q} algorithm is a \hyperlink{classAI_1_1Algorithm_1_1QLearning}{Q\+Learning} algorithm that employs a {\itshape model} to \char`\"{}model\char`\"{} the environment. Thus, in every update, the algorithm can do $n$ simulations, enabling faster convergence to \char`\"{}optimal\char`\"{} policy. 

\subsection{Constructor \& Destructor Documentation}
\hypertarget{classAI_1_1Algorithm_1_1DynaQ_aa543816270f72b2c4ac1f77fb818f792}{\index{A\+I\+::\+Algorithm\+::\+Dyna\+Q@{A\+I\+::\+Algorithm\+::\+Dyna\+Q}!Dyna\+Q@{Dyna\+Q}}
\index{Dyna\+Q@{Dyna\+Q}!A\+I\+::\+Algorithm\+::\+Dyna\+Q@{A\+I\+::\+Algorithm\+::\+Dyna\+Q}}
\subsubsection[{Dyna\+Q}]{\setlength{\rightskip}{0pt plus 5cm}template$<$class S , class A $>$ {\bf A\+I\+::\+Algorithm\+::\+Dyna\+Q}$<$ S, A $>$\+::{\bf Dyna\+Q} (
\begin{DoxyParamCaption}
\item[{{\bf A\+I\+::\+F\+L\+O\+A\+T}}]{step\+Size, }
\item[{{\bf A\+I\+::\+F\+L\+O\+A\+T}}]{discount\+Rate, }
\item[{{\bf Policy\+::\+Policy}$<$ S, A $>$ \&}]{policy, }
\item[{{\bf A\+I\+::\+U\+I\+N\+T}}]{simulation\+Iteration\+Count, }
\item[{{\bf A\+I\+::\+F\+L\+O\+A\+T}}]{state\+Transition\+Greediness, }
\item[{{\bf A\+I\+::\+F\+L\+O\+A\+T}}]{state\+Transition\+Step\+Size}
\end{DoxyParamCaption}
)}}\label{classAI_1_1Algorithm_1_1DynaQ_aa543816270f72b2c4ac1f77fb818f792}

\begin{DoxyParams}{Parameters}
{\em step\+Size} & range $[0.0, 1.0]$. High step size means faster learning, but less precise convergence. \\
\hline
{\em discount\+Rate} & range $[0.0, 1.0]$. High discount rate means more consideration of future events. \\
\hline
{\em policy} & online policy, that is policy used for action selection. \\
\hline
{\em simulation\+Iteration\+Count} & number of simulations per update/backup. \\
\hline
{\em state\+Transition\+Greediness} & greediness in selecting highest value model. \\
\hline
{\em state\+Transition\+Step\+Size} & how fast does a model update a value of a state-\/action pair. \\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\hypertarget{classAI_1_1Algorithm_1_1DynaQ_a4542226b17db4ed8a2c5ec17d37dc42f}{\index{A\+I\+::\+Algorithm\+::\+Dyna\+Q@{A\+I\+::\+Algorithm\+::\+Dyna\+Q}!update@{update}}
\index{update@{update}!A\+I\+::\+Algorithm\+::\+Dyna\+Q@{A\+I\+::\+Algorithm\+::\+Dyna\+Q}}
\subsubsection[{update}]{\setlength{\rightskip}{0pt plus 5cm}template$<$class S , class A $>$ void {\bf A\+I\+::\+Algorithm\+::\+Dyna\+Q}$<$ S, A $>$\+::update (
\begin{DoxyParamCaption}
\item[{const {\bf State\+Action}$<$ S, A $>$ \&}]{current\+State\+Action, }
\item[{const S \&}]{next\+State, }
\item[{const {\bf F\+L\+O\+A\+T}}]{reward, }
\item[{const set$<$ A $>$ \&}]{action\+Set}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [virtual]}}}\label{classAI_1_1Algorithm_1_1DynaQ_a4542226b17db4ed8a2c5ec17d37dc42f}
Update the state\+Action map. 
\begin{DoxyParams}{Parameters}
{\em current\+State} & current\+State agent is in. \\
\hline
{\em current\+Action} & action taken by agent by being in current\+State. \\
\hline
{\em next\+State} & next\+State by taking current\+Action in current\+State. \\
\hline
{\em reward} & reward of current\+State\+Action. \\
\hline
{\em action\+Set} & A set of all actions. \\
\hline
\end{DoxyParams}


Reimplemented from \hyperlink{classAI_1_1Algorithm_1_1ReinforcementLearning_a25d7fa245a79e61061436dc0f1db90cb}{A\+I\+::\+Algorithm\+::\+Reinforcement\+Learning$<$ S, A $>$}.



Reimplemented in \hyperlink{classAI_1_1Algorithm_1_1DynaQPrioritizeSweeping_ad08b55f3cf927189dd31abf9fc1c2959}{A\+I\+::\+Algorithm\+::\+Dyna\+Q\+Prioritize\+Sweeping$<$ S, A $>$}, and \hyperlink{classAI_1_1Algorithm_1_1DynaQET_a53b0e06842fbb802acfa5384a84ad448}{A\+I\+::\+Algorithm\+::\+Dyna\+Q\+E\+T$<$ S, A $>$}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
Algorithms/\+Reinforcement\+Learning/Dyna\+Q.\+h\end{DoxyCompactItemize}
